{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.messages import AIMessage , HumanMessage , BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START , StateGraph\n",
    "from langchain_community.document_loaders.text import TextLoader\n",
    "from langchain_community.document_loaders.pdf import PDFMinerLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader , Docx2txtLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from typing_extensions import TypedDict , List , Annotated\n",
    "from typing import Sequence\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "# from FlagEmbedding import BGEM3FlagModel\n",
    "from pprint import pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OllamaLLM(\n",
    "    model = \"gemma3\", # name of a loaded model\n",
    "    num_gpu = -1,\n",
    "    temperature = 1.0,\n",
    "    top_k = 64,\n",
    "    top_p = 0.95,\n",
    "    min_p = 0.0,\n",
    "    # base_url = '0.0.0.0:11444',\n",
    "    num_predict = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
    "\n",
    "# vector_store = InMemoryVectorStore(embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/miniconda3/envs/sunrise/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embed = HuggingFaceEmbeddings(model_name = r\"/media/j/msmj/models/bge m3 embedder/BAAIbge-m3\" ,\n",
    "                              encode_kwargs = {\"normalize_embeddings\": True}) # loading embedding from local\n",
    "\n",
    "vector_store = InMemoryVectorStore(embed) # loading embedding into vector store in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_loader = DirectoryLoader(path = \"sources/\" , glob = '**/*.txt' , use_multithreading = True , loader_cls = TextLoader) # loading all txt files\n",
    "docs = txt_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docx_loader = DirectoryLoader(path = \"sources/\" , glob = '**/*.docx' , use_multithreading = True , loader_cls = Docx2txtLoader) # loading all .docx files\n",
    "temp = docx_loader.load()\n",
    "docs += temp\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_loader = DirectoryLoader(path = \"sources/\" , glob = '**/*.pdf' , use_multithreading = True , loader_cls = PDFMinerLoader) # loading all pdf files\n",
    "temp = pdf_loader.load()\n",
    "docs += temp\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000 , chunk_overlap = 200) # split all documents in chunks\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorized_doc = FAISS.from_documents(all_splits , embed) # Return VectorStore initialized from documents and embeddings.\n",
    "retriever = vectorized_doc.as_retriever() # Return VectorStoreRetriever initialized from this VectorStore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# template = \"\"\"if user said hi or hello, say hi or hello back to him.\n",
    "# Use the following pieces of context to answer the question at the end.\n",
    "# It's important to pay attention to the chat history and summarized documents.\n",
    "# Your answer must be based on these pieces of context, chat history,summarized documents, and your own knowledge base.\n",
    "# If you don't know the answer from the provided context, feel free to use your internal knowledge to answer.\n",
    "# Use three sentences maximum and keep the answer as concise as possible.\n",
    "# If the user asks about his previous questions, respond **only using the chat history below** and **do not use the current question's context or documents**.\n",
    "\n",
    "# context:\n",
    "# {context}\n",
    "\n",
    "# summarized documents:\n",
    "# {summarized_doc}\n",
    "\n",
    "# chat history:\n",
    "# {chat_history}\n",
    "\n",
    "# Question: {input}\n",
    "\n",
    "# Helpful Answer:\"\"\"\n",
    "\n",
    "# # Create a prompt template\n",
    "# prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"if user said hi or hello, say hi or hello back to him.\n",
    "Use the following pieces of context to answer the question at the end.\n",
    "It's important to pay attention to the chat history and summarized documents.\n",
    "Your answer must be based solely on these pieces of context, chat history, and summarized documents.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "\n",
    "If the user asks about his previous questions, respond **only using the chat history** and **do not use the current question's context or documents**.\n",
    "\n",
    "context:\n",
    "{context}\n",
    "\n",
    "summarized documents:\n",
    "{summarized_doc}\n",
    "\n",
    "chat history:\n",
    "{chat_history}\n",
    "\n",
    "Question: {input}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining state\n",
    "class State(TypedDict): \n",
    "    messages : Annotated[Sequence[BaseMessage] , add_messages]\n",
    "    all_docs : List[List[str]]\n",
    "\n",
    "\n",
    "\n",
    "# making ['a' , 'b' , 'c' , 'd' , 'e' , 'f'] into [('a' , 'b') , ('c' , 'd') , ('e' , 'f')]\n",
    "def making_pairs(messages_list):\n",
    "    pairs_list = [(messages_list[i] , messages_list[i + 1]) for i in range(0 , len(messages_list) , 2)]\n",
    "    return pairs_list\n",
    "\n",
    "\n",
    "\n",
    "def answering(state : State):\n",
    "    \n",
    "    # initialize \"state['all_docs']\" as a empty list if its first run\n",
    "    if 'all_docs' not in state or state['all_docs'] is None: \n",
    "        state['all_docs'] = []\n",
    "        \n",
    "\n",
    "\n",
    "    relevant_docs = retriever.invoke(state['messages'][-1].content) # retrieving related doc for user question\n",
    "    integrated_relevant_docs = \"\\n\\n\".join(doc.page_content for doc in relevant_docs) # integrating retrieved docs into a single str\n",
    "\n",
    "\n",
    "    QA_pairs = making_pairs(state['messages'][-11:-1]) # make pairs of human question and ai answer\n",
    "    state['all_docs'] += [[integrated_relevant_docs]] # adding retrieved integrated docs to \"state['all_docs']\" for having history of retrieved docs\n",
    "\n",
    "\n",
    "\n",
    "    chat_history = \"\"\n",
    "    for i, (Q, A) in enumerate(QA_pairs , start = 1):  # Limit to last 5 pairs\n",
    "        chat_history += f\"Qestion {i} : {Q.content}\\nAnswer {i} : {A.content}\\n\\n\" # run this on a list of human and ai messages to see what it does\n",
    "\n",
    "\n",
    "\n",
    "    # using llm to get a summary of last 3 docs exept for latest cuz we gonna give the latest docs as related docs to user question into the prompt context\n",
    "    if state['all_docs'][-4:-1] == []:\n",
    "        summarized_docs = llm.invoke(f'concisely summarize these documents: {integrated_relevant_docs}')\n",
    "    else:\n",
    "        summarized_docs = llm.invoke(f'concisely summarize these documents: {state['all_docs'][-4:-1]}')\n",
    "\n",
    "\n",
    "\n",
    "    # fit user latest question and related retrieved docs into prompt\n",
    "    msg = prompt.invoke({'input' : state['messages'][-1].content,\n",
    "                         'context' : integrated_relevant_docs,\n",
    "                         'summarized_doc' : summarized_docs,\n",
    "                         'chat_history' : chat_history})\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    ans = llm.invoke(msg.to_messages())\n",
    "    return {'messages' : [AIMessage(ans)] , 'all_docs' : state['all_docs']} # we're returning \"state['all_docs']\" becuz it was needed for history of docs to persist\n",
    "\n",
    "\n",
    "\n",
    "memory = MemorySaver() # needed for our RAG to have a memory of interactions\n",
    "\n",
    "graph_builder = StateGraph(state_schema = State)\n",
    "graph_builder.add_edge(START, \"search\")\n",
    "graph_builder.add_node(\"search\" , answering)\n",
    "\n",
    "graph = graph_builder.compile(checkpointer = memory) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = {\"configurable\": {\"thread_id\": \"J\"}} # this link the history of our interactions with thread_id\n",
    "query = \"this is where you gotta put your questions in\"\n",
    "input_msg = [HumanMessage(query)]\n",
    "\n",
    "output = graph.invoke({\"messages\" : input_msg} , config = configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp(output['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output['messages'][-2].pretty_print()\n",
    "output['messages'][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sunrise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
